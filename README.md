# Pixel Art Character Generation

A PyTorch implementation of a flow-based generative model for creating pixel 
art characters. This project uses continuous normalizing flows with a U-Net 
architecture to learn the data distribution and generate high-quality pixel art sprites.

## ğŸ¨ Overview
This project implements a flow-based generative model that learns to transform simple 
noise distributions into complex pixel art characters. The model uses:

- **Flow-based architecture**: Continuous normalizing flows for high-quality generation
- **U-Net backbone**: Time-conditioned U-Net for learning the vector field
- **RGBA support**: Full transparency support for pixel art sprites
- **Flexible training**: Configurable noise schedules and training parameters

## ğŸš€ Features
- **Multiple noise schedules**: Linear and cosine scheduling options
- **Evaluation metrics**: FID (FrÃ©chet Inception Distance) for quality assessment
- **Training utilities**: Learning rate scheduling, checkpointing, and visualization
- **Flexible sampling**: Support for different generation modes and timesteps
- **Experiment tracking**: CSV logging and training visualization tools

## ğŸ“‹ Requirements
Install the required dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ images/                    # Your pixel art dataset (RGBA images)
â”‚   â””â”€â”€ image_only_dataset.py      # Dataset loading utilities
â”œâ”€â”€ diff_eq/
â”‚   â”œâ”€â”€ ode_sde.py                 # ODE/SDE implementations
â”‚   â””â”€â”€ simulator.py               # Numerical integration methods
â”œâ”€â”€ models/
â”‚   â””â”€â”€ unet.py                    # U-Net architecture with time conditioning
â”œâ”€â”€ sampling/
â”‚   â”œâ”€â”€ conditional_probability_path.py  # Flow probability paths
â”‚   â”œâ”€â”€ noise_scheduling.py        # Alpha/beta scheduling functions
â”‚   â””â”€â”€ sampleable.py             # Dataset samplers and distributions
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ evaluation.py             # Evaluation metrics (FID)
â”‚   â”œâ”€â”€ lr_scheduling.py          # Learning rate schedulers
â”‚   â”œâ”€â”€ objective.py              # Training objectives
â”‚   â”œâ”€â”€ trainer.py                # Main training loop
â”‚   â””â”€â”€ experiments/              # Training outputs and checkpoints
â””â”€â”€ utils/
    â”œâ”€â”€ helpers.py                # Utility functions
    â””â”€â”€ visualization.py          # Training visualization tools
```


## ğŸ“Š Monitoring Training

The training process automatically logs:

- **Training loss**: Flow matching objective loss
- **Validation FID**: FrÃ©chet Inception Distance on validation set
- **Generated samples**: Saved periodically for visual inspection
- **Model checkpoints**: Best model based on validation FID

Logs are saved to `training/experiments/unet/training_log.csv` and can be visualized using the built-in visualization tools.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.